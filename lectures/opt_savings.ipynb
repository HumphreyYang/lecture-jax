{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed48c679",
   "metadata": {},
   "source": [
    "# 10. Optimal Savings\n",
    "\n",
    "In addition to what’s in Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c113e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8e633",
   "metadata": {},
   "source": [
    "We will use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3c5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abb124",
   "metadata": {},
   "source": [
    "# 10.1. Overview\n",
    "\n",
    "We consider an optimal savings problem with CRRA utility and budget constraint\n",
    "\n",
    "$$ W_{t+1} + C_t \\leq R W_t + Y_t $$\n",
    "\n",
    "We assume that labor income $(Y_t)$ is a discretized AR(1) process.\n",
    "\n",
    "The right-hand side of the Bellman equation is \n",
    "\n",
    "$$   B((w, y), w', v) = u(Rw + y - w') + β \\sum_{y'} v(w', y') Q(y, y'). $$\n",
    "\n",
    "where\n",
    "\n",
    "$$   u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4880b15",
   "metadata": {},
   "source": [
    "We use successive approximation for VFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98142cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_approx(T,                     # Operator (callable)\n",
    "                      x_0,                   # Initial condition\n",
    "                      tolerance=1e-6,        # Error tolerance\n",
    "                      max_iter=10_000,       # Max iteration bound\n",
    "                      print_step=25,         # Print at multiples\n",
    "                      verbose=False):        \n",
    "    x = x_0\n",
    "    error = tolerance + 1\n",
    "    k = 1\n",
    "    while error > tolerance and k <= max_iter:\n",
    "        x_new = T(x)\n",
    "        error = np.max(np.abs(x_new - x))\n",
    "        if verbose and k % print_step == 0:\n",
    "            print(f\"Completed iteration {k} with error {error}.\")\n",
    "        x = x_new\n",
    "        k += 1\n",
    "    if error > tolerance:\n",
    "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
    "    elif verbose:\n",
    "        print(f\"Terminated successfully in {k} iterations.\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ef3da",
   "metadata": {},
   "source": [
    "# 10.2. Model primitives\n",
    "\n",
    "Here’s a `namedtuple` definition for storing parameters and grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbcd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', \n",
    "                    ('β', 'R', 'γ', 'w_grid', 'y_grid', 'Q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73bdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_model_jax():\n",
    "    \"Build a JAX-compatible version of the consumption model.\"\n",
    "\n",
    "    model = create_consumption_model()\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "\n",
    "    # Break up parameters into static and nonstatic components\n",
    "    constants = β, R, γ\n",
    "    sizes = len(w_grid), len(y_grid)\n",
    "    arrays = w_grid, y_grid, Q\n",
    "\n",
    "    # Shift arrays to the device (e.g., GPU)\n",
    "    arrays = tuple(map(jax.device_put, arrays))\n",
    "    return constants, sizes, arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68af5d9",
   "metadata": {},
   "source": [
    "Here's the right hand side of the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b66c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(v, constants, sizes, arrays):\n",
    "    \"\"\"\n",
    "    A vectorized version of the right-hand side of the Bellman equation \n",
    "    (before maximization), which is a 3D array representing\n",
    "\n",
    "        B(w, y, w′) = u(Rw + y - w′) + β Σ_y′ v(w′, y′) Q(y, y′)\n",
    "\n",
    "    for all (w, y, w′).\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack \n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    # Compute current rewards r(w, y, wp) as array r[i, j, ip]\n",
    "    w  = jnp.reshape(w_grid, (w_size, 1, 1))    # w[i]   ->  w[i, j, ip]\n",
    "    y  = jnp.reshape(y_grid, (1, y_size, 1))    # z[j]   ->  z[i, j, ip]\n",
    "    wp = jnp.reshape(w_grid, (1, 1, w_size))    # wp[ip] -> wp[i, j, ip]\n",
    "    c = R * w + y - wp\n",
    "\n",
    "    # Calculate continuation rewards at all combinations of (w, y, wp)\n",
    "    v = jnp.reshape(v, (1, 1, w_size, y_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
    "    Q = jnp.reshape(Q, (1, y_size, 1, y_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
    "    EV = jnp.sum(v * Q, axis=3)                 # sum over last index jp\n",
    "\n",
    "    # Compute the right-hand side of the Bellman equation\n",
    "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f87948",
   "metadata": {},
   "source": [
    "# 10.3. Operators\n",
    "\n",
    "Now we define the policy operator $T_\\sigma$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a0c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_σ(v, σ, constants, sizes, arrays):\n",
    "    \"The σ-policy operator.\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, constants, sizes, arrays)\n",
    "\n",
    "    # Compute the array v[σ[i, j], jp]\n",
    "    yp_idx = jnp.arange(y_size)\n",
    "    yp_idx = jnp.reshape(yp_idx, (1, 1, y_size))\n",
    "    σ = jnp.reshape(σ, (w_size, y_size, 1))\n",
    "    V = v[σ, yp_idx]      \n",
    "\n",
    "    # Convert Q[j, jp] to Q[i, j, jp] \n",
    "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
    "\n",
    "    # Calculate the expected sum Σ_jp v[σ[i, j], jp] * Q[i, j, jp]\n",
    "    Ev = np.sum(V * Q, axis=2)\n",
    "\n",
    "    return r_σ + β * np.sum(V * Q, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fe11f",
   "metadata": {},
   "source": [
    "and the Bellman operator $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(v, constants, sizes, arrays):\n",
    "    \"The Bellman operator.\"\n",
    "    return jnp.max(B(v, constants, sizes, arrays), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76b1a0",
   "metadata": {},
   "source": [
    "The next function computes a $v$-greedy policy given $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19fe8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy(v, constants, sizes, arrays):\n",
    "    \"Computes a v-greedy policy, returned as a set of indices.\"\n",
    "    return jnp.argmax(B(v, constants, sizes, arrays), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc27d1f",
   "metadata": {},
   "source": [
    "The function below computes the value $v_\\sigma$ of following policy $\\sigma$.\n",
    "\n",
    "The basic problem is to solve the linear system\n",
    "\n",
    "$$ v(w,y ) = u(Rw + y - \\sigma(w, y)) + β \\sum_{y'} v(\\sigma(w, y), y') Q(y, y) $$\n",
    "\n",
    "for $v$.  \n",
    "\n",
    "It turns out to be helpful to rewrite this as \n",
    "\n",
    "$$ v(w,y) = r(w, y, \\sigma(w, y)) + β \\sum_{w', y'} v(w', y') P_\\sigma(w, y, w', y') $$\n",
    "\n",
    "where $P_\\sigma(w, y, w', y') = 1\\{w' = \\sigma(w, y)\\} Q(y, y')$.\n",
    "\n",
    "We want to write this as $v = r_\\sigma + P_\\sigma v$ and then solve for $v$\n",
    "\n",
    "Note, however,\n",
    "\n",
    "* $v$ is a 2 index array, rather than a single vector.  \n",
    "* $P_\\sigma$ has four indices rather than 2 \n",
    "\n",
    "The code below \n",
    "\n",
    "1. reshapes $v$ and $r_\\sigma$ to 1D arrays and $P_\\sigma$ to a matrix\n",
    "2. solves the linear system\n",
    "3. converts back to multi-index arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d77420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(σ, constants, sizes, arrays):\n",
    "    \"Get the value v_σ of policy σ by inverting the linear map R_σ.\"\n",
    "\n",
    "    # Unpack \n",
    "    β, R, γ = constants\n",
    "    w_size, y_size = sizes\n",
    "    w_grid, y_grid, Q = arrays\n",
    "\n",
    "    r_σ = compute_r_σ(σ, constants, sizes, arrays)\n",
    "\n",
    "    # Reduce R_σ to a function in v\n",
    "    partial_R_σ = lambda v: R_σ(v, σ, constants, sizes, arrays)\n",
    "\n",
    "    return jax.scipy.sparse.linalg.bicgstab(partial_R_σ, r_σ)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55adab",
   "metadata": {},
   "source": [
    "# 10.4. Solvers\n",
    "\n",
    "Now we define the solvers, which implement VFI, HPI and OPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2629682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(model, tol=1e-5):\n",
    "    \"Implements VFI.\"\n",
    "\n",
    "    constants, sizes, arrays = model\n",
    "    _T = lambda v: T(v, constants, sizes, arrays)\n",
    "    vz = jnp.zeros(sizes)\n",
    "\n",
    "    v_star = successive_approx(_T, vz, tolerance=tol)\n",
    "    return get_greedy(v_star, constants, sizes, arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e467b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(model):\n",
    "    \"Howard policy iteration routine.\"\n",
    "    constants, sizes, arrays = model\n",
    "    vz = jnp.zeros(sizes)\n",
    "    σ = jnp.zeros(sizes, dtype=int)\n",
    "    i, error = 0, 1.0\n",
    "    while error > 0:\n",
    "        v_σ = get_value(σ, constants, sizes, arrays)\n",
    "        σ_new = get_greedy(v_σ, constants, sizes, arrays)\n",
    "        error = jnp.max(np.abs(σ_new - σ))\n",
    "        σ = σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error {error}.\")\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7925490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_policy_iteration(model, tol=1e-5, m=10):\n",
    "    \"Implements the OPI routine.\"\n",
    "    constants, sizes, arrays = model\n",
    "    v = jnp.zeros(sizes)\n",
    "    error = tol + 1\n",
    "    while error > tol:\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, constants, sizes, arrays)\n",
    "        for _ in range(m):\n",
    "            v = T_σ(v, σ, constants, sizes, arrays)\n",
    "        error = jnp.max(np.abs(v - last_v))\n",
    "    return get_greedy(v, constants, sizes, arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94479058",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
